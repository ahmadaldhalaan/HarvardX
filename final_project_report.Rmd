---
title: "Predicting Students' Performance in HarvardX Courses"
date: "8/6/2020"
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(MASS)
library(psych)
library(leaps)
library(lmtest)
library(faraway)
library(epiDisplay)
library(boot)
library(caret)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
all_courses = read.csv("harvardx_clean_v2.csv")
```
```{r echo=FALSE}
get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```

# 1.0 Introduction

In this report, we analyze the HarvardX Person-Course Academic Year 2013 De-Identified dataset, which can be downloaded [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/26147) and is also included in this submission. The dataset contains data from five online courses offered by Harvard on the edX platform (HarvardX) during the terms of Fall 2012, Spring 2013, and Summer 2013. Each observation in the dataset represents a particular student who registered for a particular course, and contains data about their demographics, their interactions with the course material, and their final grade in the course.

We chose to work with an online education dataset both because we are Online MCS students (so we have firsthand experience with online courses) and because we both have an interest in education and edtech--one of us is a former public school teacher and the other a former educational consultant.

The goal of our analysis is to create a predictive model for a student's final grade (our response variable) using the information available in the dataset about the student and their interactions with the course material. Because online courses can differ in structure and grading policies, we create separate models for separate courses. 

As a proof of concept, and to condense our analysis, we limit the scope of this report to two of the five courses in the dataset. In our data, one predictor variable (number of video plays) is present for some courses and absent for others. Therefore, we chose to analyze one course that has data available for this predictor (**PH278x**, "Human Health and Global Environmental Change") and one course that does not (**CB22x**, "The Ancient Greek Hero"). Analyses similar to what we demonstrate here could be carried out for the other courses in the dataset.

The predictive model developed for each course could be of interest to a course administrator, who could use it in future offerings of the course. The model could be used to identify students who are at risk of failing the course based on their available and incoming data, and administrators could then intervene early in the course and provide additional support or guidance to these students. Therefore, in our analysis, we are most interested in models that capture low scores, as false positives pose minimal risk (students on a successful trajectory being flagged as at risk of failing).

# 2.0 Methods

## 2.1 Dataset Description

The data file contains **338,224 observations**, each of which represents one student's demographic information, interactions with course materials, and final grade in a particular course. There are **11 variables of interest** in the dataset:

* **Categorical variables**: demographic information about the student (year of birth, gender, country/region of residence, level of education) and a dummy variable (explored) that indicates whether the student accessed more than half the course material or not

* **Numeric variables**: data about the student's activity during the course (number of total events, number of days active, number of video plays, number of forum posts, number of chapters accessed)

* **Response variable**: the student's final grade in the course, from 0.0 (0%) to 1.0 (100%)

The table below provides descriptions of all variables of interest present in the initial dataset:

| Variable          | Meaning                                                                            |
|-------------------|------------------------------------------------------------------------------------|
| explored          | Whether student interacted with at least half of course modules (1) or did not (0) |
| final_cc_cname_DI | Student's country or region                                                        |
| LoE_DI            | Student's level of education                                                       |
| YoB               | Student's year of birth                                                            |
| gender            | Student's gender                                                                   |
| nevents           | Number of course interaction events  logged                                        |
| ndays_act         | Number of days student was active in the course                                    |
| nchapters         | Number of course chapters student interacted with                                  |
| nforum_posts      | Number of forum posts by student                                                   |
| nplay_video       | Number of video plays by student                                                   |
| grade             | Student's final grade in the course                                                |

## 2.2 Data Preparation

We began by investigating our dataset to determine any data cleaning steps we should take before beginning to build models.

One issue that was immediately apparent was missing data: the dataset contained many blanks both in the demographic variables (such as gender and year of birth) and in "activity" variables (such as number of video plays). According to our data source's documentation, missing data in demographic columns indicated that student did not provide that optional information during course registration. Missing values in "activity" columns were considered zeros, meaning that the student never interacted with the course in the specified manner. For some courses in the data, values in the `nplay_video` column (number of video plays) were missing for every single student, indicating that the course did not collect that information. In at least one course, this was the case because key course material was hosted on a separate website.

Another interesting pattern we noticed was that the majority of students who registered for each course never viewed the course material at all (the `viewed` column was 0). 

The primary goal and use case of our model is to be able to identify students at risk of low grades in future iterations of the course, for the purpose of being able to intervene with these students. We decided to only consider students who interacted in some way with the course material, and to drop observations for students who never interacted with the course. We took the following steps to remove the observations of students who were completely inactive:

* Removed all observations where the student never viewed the course materials by subsetting to observations with `viewed == 1`

* Removed all observations where the student had no events logged by subsetting to observations with `nevents > 0`

* Removed all observations where the student had not accessed any chapters of the course materials by subsetting to observations with `nchapters > 0`

After taking these steps, there were still observations with missing values for demographic data such as gender, level of education, and year of birth. We also wanted to simplify the year of birth and country of residence columns, which we would treat as factor variables, to reduce the number of levels of each variable. To deal with these issues, we took the following steps:

* Added a new column, `age_band`, as a factor with three levels: Under 25, 25-30, and Over 30, calculated based on the year of birth in each observation. This split resulted in three fairly evenly-sized groups. We used this column in our models instead of using the "year of birth" column directly.

* Added a new column, `us_based`, as a factor with levels 1 (indicating the user was based in the US) and 0 (indicating that they were not). We used this column in our models instead of using the country/region column directly.

* Created a new level called "Unknown" for all missing data in the demographic columns due to the student choosing not to share this information. This was a better alternative than removing these observations as we did not want to lose power by reducing the sample size, and many active students chose not to provide this information.

Finally, because each online course is a distinct experience with varying material, assignments, and grading policies, we split the data by course. As mentioned previously, one of the variables, `nplay_video` (number of video plays) was available for some courses, but other courses did not collect this data. For this report, we chose to focus our analysis on one course where this data was collected (PH278x, "Human Health and Global Environmental Change") and another course where it was not collected (CB22x, "The Ancient Greek Hero").

## 2.3 Data Exploration

While we analyzed and modeled each course's data separately, we followed a similar process for both. The first step for each course was to isolate the variables of interest, summarize them, and visualize correlations between them. This helped us to get a sense of overall trends present in the data, variables that could potentially serve as helpful predictors, and multicollinearity among variables.

### 2.3.1 CB22x ("The Ancient Greek Hero")

We began our exploration for CB22x by selecting all variables of interest for the course and printing a summary.

```{r}
# Extract data for CB22x
cb_22 <- subset(all_courses, course_id == "HarvardX/CB22x/2013_Spring")
# Convert integer categorical variables to factor
cb_22$explored <- as.factor(cb_22$explored)
cb_22$us_based <- as.factor(cb_22$us_based)

# Choose columns of interest
cb_22 <- cb_22[,c(6,9,11,15,16,18,19,22,23,12)]

# Summarize the course variables
summary(cb_22)
```
From this summary, one thing that stood out to us was how skewed towards lower values many of the variables seemed to be. For example, our response variable, `grade`, has a median of 0.0 (indicating 0% in the course), even after we removed all completely inactive students during data preparation! Other variables, such as nforum_posts and nchapters, seem to follow similar patterns: most students accessed only one or 2 chapters, and most did not make any forum posts.

We also notice a maximum nevents of 31,943. Plotting a strip chart, we see how unusal this observation is. Since we do not know the source of this outlier, we decided to leave this observation in the analysis. Further outlier detection methods will determine if this observation is in fact influencing our model.

```{r}
# Show unusal nevents point
stripchart(cb_22$nevents, method = "jitter", col="dodgerblue", xlab="nevents")
```

Next we visualized scatterplots between each pair of variables in the dataset to get a feel for which variables might be useful predictors for `grade` and which predictors had high collinearity:

```{r fig.width=10, fig.height = 10}
# visualize correlations, distributions and x-y plots
pairs.panels(cb_22)
```

Based on this plot, we notice that four variables are highly positively correlated with our response variable, `grade`: `explored`, which takes the value 1 if a student explored more than half of the course modules and 0 if not, `nevents` (number of interaction events), `ndays_act` (number of days active), and `nchapters` (number of chapters accessed). This makes intuitive sense: we would have guessed that students who are more active and access more course materials tend to get better grades.

It is worth noting that these variables are also highly positively correlated with each other, indicating possible multicollinearity. However, because our goal is to create the best predictive model for this course, rather than an explanatory model, we are not particularly concerned with this.

### 2.3.2 PH278x ("Human Health and Global Environmental Change")

We followed the same process for the other course we modeled, PH278x, starting by selecting and summarizing variables of interest:

```{r}
# Extract data for PH278x
ph_278 <- subset(all_courses, course_id == "HarvardX/PH278x/2013_Spring")

# Convert integer categorical variables to factor
ph_278$explored <- as.factor(ph_278$explored)
ph_278$us_based <- as.factor(ph_278$us_based)

# Choose columns of interest
ph_278 <- ph_278[,c(6,9,11,15,16,17,18,19,22,23,12)]

# Summarize the course variables
summary(ph_278)
```
Here we notice many of the same trends we saw in CB22x: several of the variables, like `grade`, `nchapters`, `nplay_video`, and `nforum_posts` seem to be skewed, with low values for the majority of observations.

Next, we visualized scatterplots between the variables to look for potentially useful predictors for `grade`:

```{r fig.width = 10, fig.height = 10}
# Visualize correlations, distributions and x-y plots
pairs.panels(ph_278)
```
Just as with CB22x, we see that `explored`, `nchapters`, `nevents`, and `ndays_act` are all positively correlated with `grade`. The variable `nplay_video`, which was not available for CB22x, is also positively correlated with `grade` but to a lesser extent. We also see possible multicollinearity between these variables.

## 2.4 Model Building for CB22x ("The Ancient Greek Hero")

After initial data exploration for both of our courses, we began the model building process. With the goal of selecting the best predictive model for each course, we randomly split each course's data into a training set (80% of the data) and a test set (20% of the data). We fit a variety of linear models to the training data, starting with simple additive models including the predictors that seemed most promising to us. We considered models with polynomial and interaction terms, and created several more complex candidate models using backwards, forwards, and stepwise search procedures.

Finally, to select the "best" prediction model out of our candidate models for each course, we used LOOCV-RMSE as a metric, ultimately selecting the model with the lowest LOOCV-RMSE.

### 2.4.1 Modelling

```{r}
# Split data into train and test sets
set.seed(420)
cb_22_trn_idx  <- sample(nrow(cb_22), size = trunc(0.80 * nrow(cb_22)))
cb_22_trn_data <- cb_22[cb_22_trn_idx, ]
cb_22_tst_data <- cb_22[-cb_22_trn_idx, ]
```

We started the modelling process for CB22x by creating additive models: 
```{r}
# Fit additive model with just the most promising predictors
cb_22_small <- lm(grade ~ explored + nchapters + nevents + ndays_act,
                  data = cb_22_trn_data)

# Fit full additive model
cb_22_add <- lm(grade ~., data = cb_22_trn_data)

# Compare models
summary(cb_22_small)$adj.r.squared
summary(cb_22_add)$adj.r.squared
anova(cb_22_small, cb_22_add)
```
Based on the adjusted R squared values and the results of the ANOVA F-test, it looks like using predictors beyond `explored`, `nchapters`, `nevents`, and `ndays_act` is helpful. 

Next we considered a polynomial model, including quadratic terms for `nevents`, `ndays_act`, and `nchapters`:
```{r}
# Add higher-order terms for continuous predictors
cb_22_poly <- lm(grade ~ . + I(nevents^2) + I(ndays_act^2) + I(nchapters^2), 
                 data = cb_22_trn_data)

# Compare to full additive model
summary(cb_22_poly)$adj.r.squared
anova(cb_22_add, cb_22_poly)
```
Based on the adjusted R squared values and the results of the ANOVA F-test, these quadratic terms are also useful.

Next we considered a model including both the quadratic terms and all two-way interactions between predictors:
```{r}
# Add all two-way interaction terms
cb_22_inter <- lm(grade ~ .^2 + I(nevents^2) + I(ndays_act^2) + I(nchapters^2), 
                 data = cb_22_trn_data)
summary(cb_22_inter)$adj.r.squared
anova(cb_22_poly, cb_22_inter)
```
Based on these results, the added interaction terms are helpful as well.

### 2.4.2 Model Selection

To select additional candidate models for prediction, we used backwards, forwards, and stepwise search procedures with both AIC and BIC as metrics:
```{r}
# Fit starting model with no predictors
cb_22_start = lm(grade ~ 1, data = cb_22_trn_data)
n = length(resid(cb_22_start))
```

```{r}
# Stepwise AIC model selection
cb_22_selection_step_aic <- step(cb_22_start, 
                                 scope = grade ~ (explored + LoE_DI + gender + nevents +   
                                                  ndays_act + nchapters + nforum_posts + 
                                                  us_based + age_band)^2 + I(nevents^2) + 
                                                  I(ndays_act^2) + I(nchapters^2),
                                 direction = "both",
                                 trace = 0)
```

```{r}
# Stepwise BIC model selection
cb_22_selection_step_bic <- step(cb_22_start, 
                                 scope = grade ~ (explored + LoE_DI + gender + nevents +   
                                                  ndays_act + nchapters + nforum_posts + 
                                                  us_based + age_band)^2 + I(nevents^2) + 
                                                  I(ndays_act^2) + I(nchapters^2),
                                 direction = "both",
                                 k = log(n),
                                 trace = 0)
```

```{r}
# Forward AIC model selection
cb_22_selection_forw_aic <- step(cb_22_start, 
                                 scope = grade ~ (explored + LoE_DI + gender + nevents +   
                                                  ndays_act + nchapters + nforum_posts + 
                                                  us_based + age_band)^2 + I(nevents^2) + 
                                                  I(ndays_act^2) + I(nchapters^2),
                                 direction = "forward",
                                 trace = 0)
```

```{r}
# Forward BIC model selection
cb_22_selection_forw_bic <- step(cb_22_start, 
                                 scope = grade ~ (explored + LoE_DI + gender + nevents +   
                                                  ndays_act + nchapters + nforum_posts + 
                                                  us_based + age_band)^2 + I(nevents^2) + 
                                                  I(ndays_act^2) + I(nchapters^2),
                                 direction = "forward",
                                 k = log(n),
                                 trace = 0)
```

```{r}
# Backward AIC model selection
cb_22_selection_back_aic <- step(cb_22_inter,
                                 direction = "backward",
                                 trace = 0)
```

```{r}
# Backward BIC model selection
cb_22_selection_back_bic <- step(cb_22_inter,
                                 direction = "backward",
                                 k = log(n),
                                 trace = 0)
```

### 2.4.3 Selecting the Best Model

At this point, we had 10 candidate models for CB22x: two additive models, a quadratic model, a full interaction model with quadratic terms, and the six models selected via backward, forward, and stepwise procedures with AIC and BIC, respectively. To choose a "best" model from among these, we used LOOCV-RMSE as a metric, as we were primarily concerned with the predictive power of our chosen model and avoiding overfitting, and not concerned about model size and explainability.

```{r eval=FALSE}
# Small additive model using only four "promising" predictors
get_loocv_rmse(cb_22_small)
summary(cb_22_small)$adj.r.squared
# Full additive model
get_loocv_rmse(cb_22_add)
summary(cb_22_add)$adj.r.squared
# Polynomial model
get_loocv_rmse(cb_22_poly)
summary(cb_22_poly)$adj.r.squared
# Two-way interaction plus polynomial model
get_loocv_rmse(cb_22_inter)
summary(cb_22_inter)$adj.r.squared
# AIC stepwise selected model
get_loocv_rmse(cb_22_selection_step_aic)
summary(cb_22_selection_step_aic)$adj.r.squared
# BIC stepwise selected model
get_loocv_rmse(cb_22_selection_step_bic)
summary(cb_22_selection_step_bic)$adj.r.squared
# AIC forward selected model
get_loocv_rmse(cb_22_selection_forw_aic)
summary(cb_22_selection_forw_aic)$adj.r.squared
# BIC forward selected model
get_loocv_rmse(cb_22_selection_forw_bic)
summary(cb_22_selection_forw_bic)$adj.r.squared
# AIC backward selected model
get_loocv_rmse(cb_22_selection_back_aic)
summary(cb_22_selection_back_aic)$adj.r.squared
# BIC backward selected model
get_loocv_rmse(cb_22_selection_back_bic)
summary(cb_22_selection_back_bic)$adj.r.squared
```

The table below in section 2.4.4 shows the resulting LOOCV-RMSE as well as the adjusted $R^2$ value for each model.

### 2.4.4 Finding Influential Observations

The following table displays the LOOCV-RMSE and adjusted $R^2$ values for each of our ten candidate models. Based on the LOOCV-RMSE metric, the "best" model was our polynomial model: 

| Model                 | LOOCV-RMSE | Adjusted R^2 |
|-----------------------|------------|--------------|
| Small Additive        | 0.06003899 | 0.8145118    |
| Full Additive         | 0.0598797  | 0.8162382    |
| Polynomial            | 0.05891184 | 0.8381889    |
| Two-Way Interaction   | 0.1715243  | 0.8688724    |
| AIC Stepwise Selected | 0.1740965  | 0.8622347    |
| BIC Stepwise Selected | 0.0659327  | 0.8561416    |
| AIC Forward Selected  | 0.1740965  | 0.8622347    |
| BIC Forward Selected  | 0.06538608 | 0.8562292    |
| AIC Backward Selected | 0.1455612  | 0.868931     |
| BIC Backward Selected | 0.1437295  | 0.8684425    |


We noticed that the models that were more complex and involved many interaction terms performed much worse on the LOOCV-RMSE metric than models with fewer interaction terms, but more complicated models also generally had higher adjusted $R^2$ values. We decided to check the data for influential points using Cook's Distance, and to manually look at the most influential points to see whether they should be removed.

```{r}
# Find influential points according to Cook's Distance
cd_cb_22_poly = cooks.distance(cb_22_poly)
sum(cd_cb_22_poly > 4 / length(cd_cb_22_poly))
# Find biggest influencers
halfnorm(cooks.distance(cb_22_poly))
```

We found many influential points, but one outlier in particular, `4937`: 

```{r}
# Too many events
cb_22_trn_data[4937,]
```

Looking at this observation, it was clear that the number of events was unreasonable (almost 32,000 events, when the 3rd quartile for CB22x was on the order of 100 events). This was the outlier we had noted during our data exploration. We decided to re-fit our models without this observation to see if their performance would change.

We have chosen to omit most of this code from the report, as it is identical to the model-fitting code above with the exception of adding `subset = -4937` to all calls to `lm()`. As it turned out, removing this single point did significantly impact our results. The table below summarizes the LOOCV-RMSE and Adjusted $R^2$ values of each model after removing this single influential point from the data:

| Model                 | LOOCV-RMSE | Adjusted R^2 |
|-----------------------|------------|--------------|
| Small Additive        | 0.05569757 | 0.8244272    |
| Full Additive         | 0.05549773 | 0.8261727    |
| Polynomial            | 0.05376179 | 0.8385906    |
| Two-Way Interaction   | 0.0527604  | 0.8719732    |
| AIC Stepwise Selected | 0.05080749 | 0.8617951    |
| BIC Stepwise Selected | 0.05136239 | 0.8528922    |
| AIC Forward Selected  | 0.05080749 | 0.8617951    |
| BIC Forward Selected  | 0.0514761  | 0.8528785    |
| AIC Backward Selected | 0.0524492  | 0.8719904    |
| BIC Backward Selected | 0.05114232 | 0.871146     |

Removing this influential point from our training data also changed which model performed the best: Based on these results, and using LOOCV-RMSE as a metric, the AIC Stepwise Selected and AIC Forward Selected models, which (which turned out to be the same model), performed the best. The code used to generate this model is below:

```{r}
# Fit starting model with no predictors
cb_22_start_inf = lm(grade ~ 1, data = cb_22_trn_data, subset = -4937)
n = length(resid(cb_22_start_inf))
# Stepwise AIC model selection
cb_22_selection_step_aic_inf <- step(cb_22_start_inf, 
                                 scope = grade ~ (explored + LoE_DI + gender + nevents +   
                                                  ndays_act + nchapters + nforum_posts + 
                                                  us_based + age_band)^2 + I(nevents^2) + 
                                                  I(ndays_act^2) + I(nchapters^2),
                                 direction = "both",
                                 trace = 0)
```

This model is a fairly complex model, including several interaction and quadratic terms:
```{r}
# Show form of best model
cb_22_selection_step_aic_inf$call
```


### 2.4.5 Chosen Model Diagnostics

After choosing the "best" model, we investigated the assumptions of linearity, independence, normality, and constant variance: based on the nature of our data, in particular the fact that our response variable was bounded between 0 and 1 with many values close to either extreme, we suspected that our linear models might not satisfy these assumptions. Because we are concerned with predictive accuracy over explainability, violating these assumptions is not a "deal breaker" for our chosen models, but we were still interested in looking into whether these issues were present, and if so, if there was anything we could do to improve them.

Plotting the fitted vs residuals plot, we observe a diagonal streak in the plot, which is a result of the large number of 0 response values in the data. We see that the mean of residuals is approximatley 0, validating the linearity assumption. However, the spread of the residuals is not constant at every fitted value, with smaller fitted values having smaller residual variances. This is confirmed by the Breusch-Pagan test, which results in a p-value of approximatley 0, rejecting the null hypothesis of constant variance at all significance levels. 

```{r}
# check constant variance and linearity assumptions
plot(fitted(cb_22_selection_step_aic_inf), 
     resid(cb_22_selection_step_aic_inf), 
     xlab = "Fitted Values",
     ylab = "Residuals",
     col = "dodgerblue", 
     pch = 20)
abline(h = 0, lwd = 3, col = "gray")
bptest(cb_22_selection_step_aic_inf)
```

The Q-Q plot of the residuals does not support normality. Non-normality is confirmed by the Shapiro-Wilk test, which results in a p-value of approximatley 0, rejecting the null hypothesis of a normal distribution for the residuals.

```{r}
# Check Normality Assumption
qqnorm(resid(cb_22_selection_step_aic_inf), col = "dodgerblue")
qqline(resid(cb_22_selection_step_aic_inf), col = "gray", lwd = 3)       
```

Residuals appear to be independent based on the sequence plot.

```{r}
# Check independence assumption
plot(resid(cb_22_selection_step_aic_inf), type="l", col="grey", ylab="Residuals")
```

Non-normality and unequal variance of residuals frequently appear together in linear regression. Stabilizing the variance tends to also normalize the residuals. Therefore, we attempt to stabilize residuals' variances first and then test the normality assumption. Since the linearity assumption is met, a transformation on the response variable is most appropiate. This, of course, will be a challenge as mentioned earlier, given the concentration of low grades and the bounded nature of the reponse variable.

The Box-Cox procedure identifies the most appropiate transformation on the response variable. Box-Cox produces a $\lambda=-2$ for the transformation of the response variable.

```{r}
# Use a constant to use boxcox
c = 0.5*sort(unique(cb_22_trn_data$grade))[2]
cb_22_trn_data$c= cb_22_trn_data$grade + c

# Refit model with constant
cb_22_trans = lm(c ~ I(nchapters^2) + nevents + explored + I(ndays_act^2) + 
                    us_based + I(nevents^2) + age_band + nchapters + nevents:explored + 
                    explored:us_based + nevents:us_based + explored:age_band + 
                    nevents:age_band + us_based:nchapters + nevents:nchapters + 
                    age_band:nchapters + explored:nchapters, 
                 data = cb_22_trn_data, subset = -4937)

# Get lambda - does not help with normality/ncv
boxcox(cb_22_trans, plotit = TRUE)
```

Fitting the model with the transformed reponse variable slightly stabilizes variance and produces some normality. However, constant variance and normality assumptions are still not met. More rigorous methods, such as weighted least squares, might be needed to address these model assumptions. 

Since we are after a predictive model, we are not too concerned about violating these model assumptions. We note though that since these model assumptions have not been met, we do not give any explanatory weight to our predictors and hence our model can only be used for prediction purposes.

## 2.5 Model Building for PH278x ("Human Health and Global Environmental Change")

Our modelling process for PH278x was quite similar to the process for CB22x. The steps are detailed below.

### 2.5.1 Modelling
```{r}
# Split data into train and test sets
set.seed(420)
ph_278_trn_idx  <- sample(nrow(ph_278), size = trunc(0.80 * nrow(ph_278)))
ph_278_trn_data <- ph_278[ph_278_trn_idx, ]
ph_278_tst_data <- ph_278[-ph_278_trn_idx, ]
```

Similarly to our process for CB22x, we started the modelling process for PH278x by creating additive models: 
```{r}
# Fit additive model with just the most promising predictors
ph_278_small <- lm(grade ~ explored + nchapters + nevents + ndays_act,
                   data = ph_278_trn_data)

# Fit full additive model
ph_278_add <- lm(grade ~., data = ph_278_trn_data)

# Compare models
summary(ph_278_small)$adj.r.squared
summary(ph_278_add)$adj.r.squared
anova(ph_278_small, ph_278_add)
```
Based on the adjusted R squared values and the results of the ANOVA F-test, it looks like using predictors beyond `explored`, `nchapters`, `nevents`, and `ndays_act` is helpful. 

Next we considered a polynomial model, including quadratic terms for `nevents`, `ndays_act`, `nchapters`, and `nplay_video`:
```{r}
# Add higher-order terms for continuous predictors
ph_278_poly <- lm(grade ~ . + I(nevents^2) + I(ndays_act^2) + I(nchapters^2) +
                    I(nplay_video^2), 
                  data = ph_278_trn_data)

# Compare to full additive model
summary(ph_278_poly)$adj.r.squared
anova(ph_278_add, ph_278_poly)
```
Based on the adjusted R squared values and the results of the ANOVA F-test, these quadratic terms are also useful.

Next we considered a model including both the quadratic terms and all two-way interactions between predictors:
```{r}
# Add all two-way interaction terms
ph_278_inter <- lm(grade ~ .^2 + I(nevents^2) + I(ndays_act^2) + I(nchapters^2) +
                     I(nplay_video^2), 
                  data = ph_278_trn_data)
summary(ph_278_inter)$adj.r.squared
anova(ph_278_poly, ph_278_inter)
```
Based on these results, the added interaction terms are helpful as well.

### 2.5.2 Model Selection

As we did with CB22x, to select additional candidate models for prediction, we used backwards, forwards, and stepwise search procedures with both AIC and BIC as metrics:
```{r}
# Fit starting model with no predictors
ph_278_start = lm(grade ~ 1, data = ph_278_trn_data)
n = length(resid(ph_278_start))
```

```{r}
# Stepwise AIC model selection
ph_278_selection_step_aic <- step(ph_278_start, 
                                  scope = grade ~ (explored + LoE_DI + gender + nevents +   
                                                   ndays_act + nchapters + nforum_posts + 
                                                   us_based + age_band)^2 + I(nevents^2) + 
                                                   I(ndays_act^2) + I(nchapters^2) +
                                                   I(nplay_video^2),
                                  direction = "both",
                                  trace = 0)
```

```{r}
# Stepwise BIC model selection
ph_278_selection_step_bic <- step(ph_278_start, 
                                  scope = grade ~ (explored + LoE_DI + gender + nevents +   
                                                   ndays_act + nchapters + nforum_posts + 
                                                   us_based + age_band)^2 + I(nevents^2) + 
                                                   I(ndays_act^2) + I(nchapters^2) +
                                                   I(nplay_video^2),
                                  direction = "both",
                                  k = log(n),
                                  trace = 0)
```

```{r}
# Forward AIC model selection
ph_278_selection_forw_aic <- step(ph_278_start, 
                                  scope = grade ~ (explored + LoE_DI + gender + nevents +   
                                                   ndays_act + nchapters + nforum_posts + 
                                                   us_based + age_band)^2 + I(nevents^2) + 
                                                   I(ndays_act^2) + I(nchapters^2) +
                                                   I(nplay_video^2),
                                  direction = "forward",
                                  trace = 0)
```

```{r}
# Forward BIC model selection
ph_278_selection_forw_bic <- step(ph_278_start, 
                                  scope = grade ~ (explored + LoE_DI + gender + nevents +   
                                                   ndays_act + nchapters + nforum_posts + 
                                                   us_based + age_band)^2 + I(nevents^2) + 
                                                   I(ndays_act^2) + I(nchapters^2) +
                                                   I(nplay_video^2),
                                  direction = "forward",
                                  k = log(n),
                                  trace = 0)
```

```{r}
# Backward AIC model selection
ph_278_selection_back_aic <- step(ph_278_inter,
                                  direction = "backward",
                                  trace = 0)
```

```{r}
# Backward BIC model selection
ph_278_selection_back_bic <- step(ph_278_inter,
                                  direction = "backward",
                                  k = log(n),
                                  trace = 0)
```

### 2.5.3 Selecting the Best Model

Now that we had 10 candidate models for PH278, we used LOOCV-RMSE as a metric to select a "best" model, just as we did with CB22x:

```{r eval=FALSE}
# Small additive model using only four "promising" predictors
get_loocv_rmse(ph_278_small)
summary(ph_278_small)$adj.r.squared
# Full additive model
get_loocv_rmse(ph_278_add)
summary(ph_278_add)$adj.r.squared
# Polynomial model
get_loocv_rmse(ph_278_poly)
summary(ph_278_poly)$adj.r.squared
# Two-way interaction plus polynomial model
get_loocv_rmse(ph_278_inter)
summary(ph_278_inter)$adj.r.squared
# AIC stepwise selected model
get_loocv_rmse(ph_278_selection_step_aic)
summary(ph_278_selection_step_aic)$adj.r.squared
# BIC stepwise selected model
get_loocv_rmse(ph_278_selection_step_bic)
summary(ph_278_selection_step_bic)$adj.r.squared
# AIC forward selected model
get_loocv_rmse(ph_278_selection_forw_aic)
summary(ph_278_selection_forw_aic)$adj.r.squared
# BIC forward selected model
get_loocv_rmse(ph_278_selection_forw_bic)
summary(ph_278_selection_forw_bic)$adj.r.squared
# AIC backward selected model
get_loocv_rmse(ph_278_selection_back_aic)
summary(ph_278_selection_back_aic)$adj.r.squared
# BIC backward selected model
get_loocv_rmse(ph_278_selection_back_bic)
summary(ph_278_selection_back_bic)$adj.r.squared
```

### 2.4.4 Finding Influential Observations

Many of our candidate models had similar LOOCV-RMSE and Adjusted $R^2$ scores. These are summarized in the table below:

| Model                 | LOOCV-RMSE | Adjusted R^2 |
|-----------------------|------------|--------------|
| Small Additive        | 0.1044799  | 0.7140915    |
| Full Additive         | 0.1031384  | 0.7224598    |
| Polynomial            | 0.09994651 | 0.7401845    |
| Two-Way Interaction   | 0.104109   | 0.7627554    |
| AIC Stepwise Selected | 0.1001803  | 0.7477485    |
| BIC Stepwise Selected | 0.1010837  | 0.7390994    |
| AIC Forward Selected  | 0.1001803  | 0.7477485    |
| BIC Forward Selected  | 0.1010837  | 0.7390994    |
| AIC Backward Selected | 0.1001348  | 0.7629467    |
| BIC Backward Selected | 0.09871358 | 0.7613466    |

This suggests that our selected model ("BIC Backward Selected", in the last row of the table) is not a very clear-cut winner.

This model is quite complex, including several interaction and quadratic terms:

```{r}
# Show form of best model
ph_278_selection_back_bic$call
```

Although we did not see huge discrepancies in the performances of different models, we decided to check for influential observations in this course as well:

```{r}
# Find influential points according to Cook's Distance
cd_ph_278_selection_back_bic = cooks.distance(ph_278_selection_back_bic)
sum(cd_ph_278_selection_back_bic > 4 / length(cd_ph_278_selection_back_bic))
# Find biggest influencers
halfnorm(cooks.distance(ph_278_selection_back_bic))
```

We found many influential points, with `8278` being the most influential, so we took a look at that observation: 

```{r}
# Too many events
ph_278_trn_data[8278,]
```

Similarly to our most influential observation in CB22x, this student had a high number of events. They also had a high number of days active and video plays, relative to other users. However, unlike the most influential observation in CB22x, this user was not at the "max" value for any of these categories. We made the judgment call not to remove this observation.

### 2.4.5 Chosen Model Diagnostics

Plotting the fitted vs residuals plot, we observe two diagonal streaks in the plot, representing the two boundaries of our response variable. We see that the mean of residuals is approximately 0, validating the linearity assumption. However, the spread of the residuals is not constant at every fitted value, with smaller and larger fitted values having smaller residual variances. This is confirmed by the Breusch-Pagan test, which results in a p-value of approximately 0, rejecting the null hypothesis of constant variance at all significance levels. 

```{r}
# check constant variance and linearity assumptions
plot(fitted(ph_278_selection_back_bic), 
     resid(ph_278_selection_back_bic), 
     xlab = "Fitted Values",
     ylab = "Residuals",
     col = "dodgerblue", 
     pch = 20)
abline(h = 0, lwd = 3, col = "gray")
bptest(ph_278_selection_back_bic)
```

The Q-Q plot of the residuals does not support normality. Non-normality is confirmed by the Shapiro-Wilk test, which results in a p-value of approximatley 0, rejecting the null hypothesis of a normal distribution for the residuals.

```{r}
# Check Normality Assumption
qqnorm(resid(ph_278_selection_back_bic), col = "dodgerblue")
qqline(resid(ph_278_selection_back_bic), col = "gray", lwd = 3)       
```

Residuals appear to be independent based on the sequence plot.

```{r}
# Check independence assumption
plot(resid(ph_278_selection_back_bic), type="l", col="grey", ylab="Residuals")
```

Since both constant variance and normality are not met, we employ a Box-Cox transformation on the response variable. Box-Cox produces a $\lambda=-0.5$ for the transformation of the response variable.

```{r}
# Use a constant to use boxcox
c = 0.5*sort(unique(ph_278_trn_data$grade))[2]
ph_278_trn_data$c= ph_278_trn_data$grade + c

ph_278_trans = lm(c ~ explored + LoE_DI + gender + nevents + ndays_act + 
                  nplay_video + nchapters + nforum_posts + us_based + age_band + 
                  I(nevents^2) + I(ndays_act^2) + explored:ndays_act + explored:nplay_video + 
                  explored:nforum_posts + explored:us_based + explored:age_band + 
                  LoE_DI:nevents + LoE_DI:ndays_act + LoE_DI:nchapters + gender:nchapters + 
                  nevents:ndays_act + nevents:nplay_video + nevents:age_band + 
                  ndays_act:nplay_video + nplay_video:nchapters + nplay_video:us_based + 
                  nplay_video:age_band + nchapters:nforum_posts, 
                 data = ph_278_trn_data)

boxcox(ph_278_trans, plotit = TRUE)
```

Fitting the model with the transformed reponse variable slightly stabilizes variance and produces some normality. However, constant variance and normality assumptions are still not met. Similar to our CB22x model diagnostics, since these model assumptions have not been met, we do not give any explanatory weight to our predictors and hence our model can only be used for prediction purposes.

# 3.0 Results

In this section, we will discuss the model chosen as the "best" for each course and evaluate its performance when predicting student grades using the test data. We use the mean absolute error as a metric to quantify the performance of our model. This is an appropriate metric as it penalizes errors in predictions relatively equally for high and low grades.

## 3.1 Results for CB22x, "The Ancient Greek Hero"

### 3.1.1 Summary of Chosen Model and Comparison to Other Candidates

After removing the influential observation with an unrealistic number of events, the model with the lowest LOOCV-RMSE ended up being the model chosen via stepwise AIC. The model has an adjusted $R^2$ of 0.86, meaning that most of the variation in the grades is in the model. The model summary is shown below:

```{r}
summary(cb_22_selection_step_aic_inf)
```

### 3.1.2 Chosen Model's Performance on Test Data

After settling on our chosen model for the course, we used it to predict students' grades on the test data we held out at the beginning. We then calculated the mean absolute error for all students, and then for the subsets of students who received less than a 50% in the class and more than 50% in the class.

```{r}
# Predict grades on test set
cb_22_predictions <- predict(cb_22_selection_step_aic_inf, 
                              newdata = cb_22_tst_data)

# Calculate the mean absolute error
mean(abs(cb_22_predictions - cb_22_tst_data$grade))

# Calculate the mean absolute error for grades below 50%
below_50_idx <- cb_22_tst_data$grade < 0.5
mean(abs(cb_22_predictions[below_50_idx] - cb_22_tst_data$grade[below_50_idx]))

# Calculate the mean absolute error for grades above 50%
above_50_idx <- cb_22_tst_data$grade >= 0.5
mean(abs(cb_22_predictions[above_50_idx] - cb_22_tst_data$grade[above_50_idx]))
```

The model has a mean absolute error of 0.017 for all observations in our test set. Limiting the prediction to students with actual failing grades (less than 0.5), we get a smaller mean absolute error of 0.014. Similarly, limiting the prediction to students with actual passing grades (more than 0.5), we get a much larger mean absolute error of 0.137.

The predicted grade vs actual grade plot, shown below, visualizes the model's predictions: 

```{r}
# Fitted vs. actuals plot for chosen model (CB22)
plot(cb_22_tst_data$grade, cb_22_predictions, xlab="Actual Grade", 
       ylab="Predicted Grade", col = "gray",
       main="CB22x")
abline(a=0, b=1, col="dodgerblue", lwd=3)
```

From this plot, we see that the model predicts well for students with low grades, with the exception of the vertical streak at $x=0$. These might be students with unusual activities that correlate with the activities of higher scoring students. We can also see that the model underestimates the grades for the highest scoring students above 0.8. We will discuss the implications of this in the "Discussion" section.

## 3.2 Results for PH278x, "Human Health and Global Environmental Change"

### 3.2.1 Summary of Chosen Model and Comparison to Other Candidates

The model with the lowest LOOCV-RMSE ended up being the model selected via backwards BIC. This model is very large, and includes many interaction terms as well as two quadratic terms. The model has an adjusted $R^2$ of 0.76. Because we are concerned with prediction rather than explanation, we make no inferences and examine the model's predictive abilities. The model summary is shown below:

```{r}
summary(ph_278_selection_back_bic)
```

### 3.2.2 Chosen Model's Performance on Test Data

We used the selected model to predict grades for all observations in our test data, and then calculated the mean absolute error for all students, as well as the subsets of students who earned grades below and above 50%.

```{r}
# Predict grades on test set
ph_278_predictions <- predict(ph_278_selection_back_bic, 
                              newdata = ph_278_tst_data)

# Calculate the average absolute error
mean(abs(ph_278_predictions - ph_278_tst_data$grade))

# Calculate the average absolute error for grades below 50%
below_50_idx <- ph_278_tst_data$grade < 0.5
mean(abs(ph_278_predictions[below_50_idx] - ph_278_tst_data$grade[below_50_idx]))

# Calculate the average absolute error for grades above 50%
above_50_idx <- ph_278_tst_data$grade >= 0.5
mean(abs(ph_278_predictions[above_50_idx] - ph_278_tst_data$grade[above_50_idx]))
```

The model has a mean absolute error of 0.046. Limiting the prediction to students with actual failing grades (less than 0.5), we get a lower mean absolute error of 0.035. Similarly, limiting the prediction to students with actual passing grades (more than 0.5), we get a much higher mean absolute error of 0.221.

The predicted grade vs actual grade plot below visualizes the model's predictions. 

```{r}
# Fitted vs. actuals plot for chosen model (PH278)
plot(ph_278_tst_data$grade, ph_278_predictions, xlab="Actual Grade", 
       ylab="Predicted Grade", col = "gray",
       main="PH278")
abline(a=0, b=1, col="dodgerblue", lwd=3)
```

Similar to the CB22x model, this model predicts well for low-scoring students. Again, we observe here a vertical streak at $x=0$, representing increasing errors in predictions for students with a grade of 0. Also similar to the CB22x model, we see that this model underestimates the grades for higher-grade students. In this model, the underestimation is wider (as reflected by the larger mean absolute error) starting for students with actual grades of 0.6.

This model also produces a few nonsensical results, such as a grade above 1.0 and a grade below -0.5.

# 4.0 Discussion

Our analysis shows that for the two HarvardX courses, larger, more "complicated" models have lower LOOCV RMSE values and give good predictions on test sets. Both models use interactions and higher order terms and have high adjusted $R^2$ values. The models predict well on average, with predictions having a mean error of about 0.02 for CB22x and 0.05 for PH278x. 

In both models, we see an increasing variability in predictions for higher-grade students. This is especially evident for PH278x, which has a high mean absolute error for high-scoring students at 0.22. Although the model has an additional predictor variable, `nplay_video`, that does not seem to give it better predictions. 

The models also lack explainability as they do not satisfy linear regression model assumptions. Therefore, while they are useful in predicting students' grades in a course, they would not be useful in explaining the underlying reasons for these grades.

MOOCs, as evidenced by this dataset, suffer from very poor completion rates and efforts to predict students on a failing path could be the most pressing issue at hand. Based on our results, our models perform very well for this use case: they are particularly good at "capturing" failing students, and mostly accurately identify passing students, though with larger errors (particularly for high-graded PH278x students). 

To evaluate each model's usefulness visually, we plot predicted vs actual grades for each course. We include cutoff quadrants, similar to a confusion matrix, to make it easy to identify which students actually passed/failed the course, and which students were predicted to pass/fail the course. The plot below shows this data for CB22x, "The Ancient Greek Hero":

```{r}
plot(cb_22_tst_data$grade, cb_22_predictions, xlab="Actual Grade", ylim=c(0,1),
       ylab="Predicted Grade", col = "gray",
       main="CB22x")
abline(h=0.5, lty="dashed")
abline(v=0.5, lty="dashed")
```

We see that most students predicted to fail, do indeed fail, and vice versa. Very few students are misclassified, with about nine failing students being classified as passing. 

Similar classification specificity is seen in PH278x, with only about 25 students (from 11,333 students) missclassified as passing when they actually failed:

```{r}
# Fitted vs. actuals plot for chosen model (PH278)
plot(ph_278_tst_data$grade, ph_278_predictions, xlab="Actual Grade", 
       ylab="Predicted Grade", col = "gray",
       main="PH278")
abline(h=0.5, lty="dashed")
abline(v=0.5, lty="dashed")
```

Because the assigned scope of this project was linear regression with a continuous response variable, the focus of our analysis was predicting students' final scores in the class, but for the practical purpose we would like our model to serve, we believe that a more appropriate approach would be logistic regression, to predict whether a student will pass or fail the course, rather than predicting their exact grade. 

Out of curiosity, we took some time to explore logistic regression models, and included our work on logistic regression for PH278x in the appendix below. The classification model that we developed shows high accuracy, specificity and sensitivity, confirming the appropriateness of binary classification and logistic regression for this dataset. 

# 5.0 Appendix

## 5.1 Helper function
We used this helper function from "Applied Statistics with R" throughout the project. `get_loocv_rmse` calculates the leave-one-out cross-validated RMSE.

```{r}
get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```

## 5.2 Developing a Classification Model for PH278x

We add a new variable in the dataset, called `completion`. This is a binary response variable, with students who do not complete the course (receive a grade less than 50%) being classified as "0" and students who do complete the course (receive a 50% or greater) being classified as "1".

```{r}
# split students into not completed and completed
ph_278_trn_data$completion = ifelse(ph_278_trn_data$grade < 0.5, "0", "1")
ph_278_tst_data$completion = ifelse(ph_278_tst_data$grade < 0.5, "0", "1")
ph_278_trn_data$completion = factor(ph_278_trn_data$completion)
ph_278_tst_data$completion = factor(ph_278_tst_data$completion)
```

We fit a logistic regression model using all predictor variables and select a model with the lowest BIC. For simplicity, we only consider an additive model.

```{r warning=FALSE}
# fit glm with all predictors
ph_278_class_full = glm(completion ~ . - grade - c, 
                        data = ph_278_trn_data, 
                        family = binomial)

# exhaustive search
ph_278_class_summary = summary(regsubsets(completion ~ . - grade - c, 
                                         data = ph_278_trn_data))

# choose lowest BIC
best_ind = which.min(ph_278_class_summary$bic)
ph_278_class_summary$which[best_ind, ]
```

```{r}
# selected model with lowest BIC
ph_278_class_selected = glm(completion ~ explored + nevents + ndays_act + nplay_video +
                              nforum_posts, 
                           data = ph_278_trn_data, 
                           family = binomial)
```

We then perform 5-fold cross-validation for the full additive model and the selected model. We choose the selected model as it has a slightly smaller misclassification rate.

```{r}
# 5-fold CV for full model
cv.glm(ph_278_trn_data, ph_278_class_full, K = 5)$delta[1]
```

```{r}
# 5-fold CV for selected model
cv.glm(ph_278_trn_data, ph_278_class_selected, K = 5)$delta[1]
```

Using the selected model, we predict the probability of passing the course and use a cutoff of 0.0595, which is the proportion of students who pass.

The confusion matrix shows high accuracy, sensitivity and specificity, as expected from the Discussion section. Specificity, which is what we care about the most in this use case, is at 88.15%. This high specificty indicates that the number of false positives is low (failing students predicted to pass). We are not too concerned with sensitivity, as the risk of false negatives is low. However, this adds to the model's attractivness as it generally avoids false flags and assures a course administrator that their efforts are not wasted.  

```{r}
# use proportion of completed students as cutoff
ph_278_tst_pred = ifelse(predict(ph_278_class_selected, ph_278_tst_data, type = "response") > mean(ph_278_tst_data$completion == "1"), 
                       "1", 
                       "0")
ph_278_tst_pred = factor(ph_278_tst_pred)

# confusion matrix for accuracy, sensitvity and specificity
confusionMatrix(data = ph_278_tst_pred, reference = ph_278_tst_data$completion)
```



**Authors**: Ahmad Al-Dhalaan & Samantha Squires